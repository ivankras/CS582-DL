There are many ways in which one can approach a complex problem such as image classification, and a deep understanding of all the parameters that can affect a deep learning approach is necessary.
When building a convolutional neural network, understanding how to interpret experimental results is crucial, we learned that adjusting the batch size can affect the training speed of a network, with a potential cost to accuracy, CNNs overfit rather quickly if no proper dropout parameter is set, and data normalization is extremely important for the performance of the network.

\subsection*{Future Work}
\begin{itemize}
    \item The first experiment had problems with the distillation process, from the point of view that it didn't improve very much the same model traditionally trained. Maybe more data, a different network or different hyperparameters could change that
    \item For the second experiment, results show that more training epochs may provide a more accurate model, we may look into increasing the number of epochs or introducing an early stopping criteria as we did in the first experiment
    \item Further experimentation with simpler models based on our new learnings from our models and better tuning of the hyperparameters
    \item Application of knowledge distillation on the improved model from the second experiment
\end{itemize}
